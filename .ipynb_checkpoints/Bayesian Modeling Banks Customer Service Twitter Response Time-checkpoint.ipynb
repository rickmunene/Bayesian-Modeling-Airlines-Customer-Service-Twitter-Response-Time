{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Modeling Banks Customer Service Twitter Response Time\n",
    "**Using Studentâ€™s t-distribution, Poisson distribution, Negative Binomial distribution, Hierarchical modeling and Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter is the first place people go for customer experience when interacting with service providers such as banks and telcos in Kenya. From a past study, Twitter found that customers were willing to pay more for a service provider that responded to their tweet in under six minutes.\n",
    "\n",
    "Inspired by this finding, I couldnâ€™t help but wanted to model and compare Kenyan banks customer service twitter response time especially in this Corvid-19 lockdown period.\n",
    "- I wanted to be able to answer these questions by using Bayesian Modeling:\n",
    "- Are there significant differences on customer service twitter response time among all the airlines in the data?\n",
    "- Is the weekend affect response time?\n",
    "- Do longer tweets take longer time to respond?\n",
    "- Which airline has the shortest customer service twitter response time and vice versa?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To get the data required for this exercise, we'll stream the tweets and replies from the official handles of the top banks in Kenya using Tweepy and Twitter's API.**\n",
    "\n",
    "The resulting dataset contains tweets and responses from Kenyan banks. The following data wrangling process will accomplish:\n",
    "- Get customer inquiry, and corresponding response from the company in every row.\n",
    "- Convert datetime columns to datetime data type.\n",
    "- Calculate response time to minutes.\n",
    "- Any customer inquiry that takes longer than 60 minutes will be filtered out. We are working on requests that get response in 60 minutes.\n",
    "- Create time attributes and response word count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.configdefaults): g++ not available, if using conda: `conda install m2w64-toolchain`\n",
      "C:\\Users\\Patrick Munene\\AppData\\Roaming\\Python\\Python36\\site-packages\\theano\\configdefaults.py:560: UserWarning: DeprecationWarning: there is no c++ compiler.This is deprecated and with Theano 0.11 a c++ compiler will be mandatory\n",
      "  warnings.warn(\"DeprecationWarning: there is no c++ compiler.\"\n",
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"dot\" with args ['-Tps', 'C:\\\\Users\\\\PATRIC~1\\\\AppData\\\\Local\\\\Temp\\\\tmpnqtq6o7u'] returned code: 1\n",
      "\n",
      "stdout, stderr:\n",
      " b''\n",
      "b\"'C:\\\\Users\\\\Patrick' is not recognized as an internal or external command,\\r\\noperable program or batch file.\\r\\n\"\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick Munene\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from IPython.display import Image\n",
    "import pymc3 as pm\n",
    "import arviz as az\n",
    "import scipy\n",
    "import scipy.stats as stats\n",
    "import scipy.optimize as opt\n",
    "import statsmodels.api as sm\n",
    "from sklearn import preprocessing\n",
    "import tweepy\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import demjson\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('bmh')\n",
    "colors = ['#D55E00', '#009E73', '#0072B2', '#348ABD', '#A60628', \n",
    "          '#7A68A6', '#467821', '#CC79A7', '#56B4E9', '#F0E442']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweepy really does make OAuth mostly painless. We'll need to get our credentials from Twitter Developer Account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables that contains the credentials to access Twitter API\n",
    "ACCESS_TOKEN = '37276239-Wv7d4r9amNtxITZenAe5QoPCRco7mLXiBFX4vmY1y'\n",
    "ACCESS_SECRET = '4Swxa6Z4m9AiLVWvGUK3pTUHDIB369XdcHrTl5riVBxYN'\n",
    "CONSUMER_KEY = 'SetCcBxtANQZMYyizFHC0F13O'\n",
    "CONSUMER_SECRET = 'thuJddfzmm22xxqv0RCMpwlit5COWZ1wBZP4tg4WvdgRU47BZs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup access to API\n",
    "def connect_to_twitter_OAuth():\n",
    "    auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "    auth.set_access_token(ACCESS_TOKEN, ACCESS_SECRET)\n",
    "\n",
    "    api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "    return api\n",
    "\n",
    "\n",
    "# Create API object\n",
    "api = connect_to_twitter_OAuth()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now just to make sure it worked, lets print the most recent tweets from a user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-11 07:44:10\n",
      "2020-04-19 18:26:36\n"
     ]
    }
   ],
   "source": [
    "# tweets from a specific user\n",
    "my_tweets = api.user_timeline(id='rickmunene', count=2)\n",
    "\n",
    "for tweet in my_tweets:\n",
    "    print(tweet.created_at)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can use a function to extract the attributes weâ€™re interested in and create a dataframe. Using a loop, we are able to extract tweets from multiple handles (banks in this case)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KCB Group\n",
      "@s_ngatia Welcome.  ^MD\n",
      "KCB Group\n",
      "@hametuku @richalocust That's right.  ^JT\n",
      "KCB Group\n",
      "@EddieKarume Hi, you will need to visit your branch for PIN reset.  ^JT\n",
      "KCB Group\n",
      "@hametuku @richalocust Hi, we do have a number of channels where you can easily access the funds from your account.  ^JT\n",
      "KCB Group\n",
      "@edwinnyandwaki Kindly continue transacting in the account and we shall advise you once eligible to borrow again. ^JT\n",
      "KCB Group\n",
      "@survivergoat You are welcome.  ^JT\n"
     ]
    }
   ],
   "source": [
    "for page in tweepy.Cursor(api.user_timeline, screen_name='KCBGroup', count=3).pages(2):\n",
    "    for item in page:\n",
    "        #print('https://twitter.com/' + item.in_reply_to_screen_name + '/status/' + item.in_reply_to_status_id_str)\n",
    "        print(item.user.name)\n",
    "        print(item.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "handles = ['KCBGroup','AbsaKenya','coopbankenya','NCBABankKenya','KeEquityBank','StanChartKE']\n",
    "\n",
    "import io\n",
    "\n",
    "\n",
    "merged=pd.DataFrame()\n",
    "\n",
    "for handle in handles:\n",
    "    pages = tweepy.Cursor(api.user_timeline, screen_name=handle, count=200).pages(15)\n",
    "\n",
    "    for page in pages:\n",
    "        for tweet in page:\n",
    "            print(tweet.user.name, tweet.created_at, tweet.id, tweet.in_reply_to_status_id_str, \n",
    "                  tweet.in_reply_to_screen_name, tweet.text, tweet.retweet_count, tweet.favorite_count, \n",
    "                  sep='\\t', end='\\n', file=open(\"banks_tweeter_data.txt\", \"a\",  encoding='utf-8'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names=['User', 'Created_at', 'ID', 'Reply_to_status', 'Reply_to_user', 'Tweet', 'RTs','Likes'] \n",
    "df_t1 = pd.read_csv(r'banks_tweeter_data.txt', \n",
    "                    sep='\\t', names=col_names, header=None, quoting=3, error_bad_lines=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Created_at</th>\n",
       "      <th>ID</th>\n",
       "      <th>Reply_to_status</th>\n",
       "      <th>Reply_to_user</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>RTs</th>\n",
       "      <th>Likes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>KCB Group</td>\n",
       "      <td>2020-05-31 14:29:33</td>\n",
       "      <td>1.267101e+18</td>\n",
       "      <td>1267100433887232002</td>\n",
       "      <td>s_ngatia</td>\n",
       "      <td>@s_ngatia Welcome.  ^MD</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>KCB Group</td>\n",
       "      <td>2020-05-31 14:18:36</td>\n",
       "      <td>1.267098e+18</td>\n",
       "      <td>1267096809748467712</td>\n",
       "      <td>hametuku</td>\n",
       "      <td>@hametuku @richalocust That's right.  ^JT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>KCB Group</td>\n",
       "      <td>2020-05-31 14:02:29</td>\n",
       "      <td>1.267094e+18</td>\n",
       "      <td>1267092798064517121</td>\n",
       "      <td>EddieKarume</td>\n",
       "      <td>@EddieKarume Hi, you will need to visit your branch for PIN reset.  ^JT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>KCB Group</td>\n",
       "      <td>2020-05-31 14:00:40</td>\n",
       "      <td>1.267094e+18</td>\n",
       "      <td>1267091138676129792</td>\n",
       "      <td>hametuku</td>\n",
       "      <td>@hametuku @richalocust Hi, we do have a number of channels where you can easily access the funds from your account.  ^JT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>KCB Group</td>\n",
       "      <td>2020-05-31 13:41:09</td>\n",
       "      <td>1.267089e+18</td>\n",
       "      <td>1267085281448079360</td>\n",
       "      <td>edwinnyandwaki</td>\n",
       "      <td>@edwinnyandwaki Kindly continue transacting in the account and we shall advise you once eligible to borrow again. ^JT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        User           Created_at            ID      Reply_to_status   Reply_to_user                                                                                                                     Tweet  RTs  Likes\n",
       "0  KCB Group  2020-05-31 14:29:33  1.267101e+18  1267100433887232002  s_ngatia        @s_ngatia Welcome.  ^MD                                                                                                   0.0  0.0  \n",
       "1  KCB Group  2020-05-31 14:18:36  1.267098e+18  1267096809748467712  hametuku        @hametuku @richalocust That's right.  ^JT                                                                                 0.0  0.0  \n",
       "2  KCB Group  2020-05-31 14:02:29  1.267094e+18  1267092798064517121  EddieKarume     @EddieKarume Hi, you will need to visit your branch for PIN reset.  ^JT                                                   0.0  0.0  \n",
       "3  KCB Group  2020-05-31 14:00:40  1.267094e+18  1267091138676129792  hametuku        @hametuku @richalocust Hi, we do have a number of channels where you can easily access the funds from your account.  ^JT  0.0  0.0  \n",
       "4  KCB Group  2020-05-31 13:41:09  1.267089e+18  1267085281448079360  edwinnyandwaki  @edwinnyandwaki Kindly continue transacting in the account and we shall advise you once eligible to borrow again. ^JT     0.0  1.0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_t1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Since we are interested in getting the response time to a tweet, we'll write another funtion to get the attributes of the original tweet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_ids = df_t1.Reply_to_status.tolist()\n",
    "\n",
    "#Collect all tweets & replies from the account (Original tweeet to companyh)\n",
    "def extract_tweet2_attributes(pages):\n",
    "    df_t2 = pd.DataFrame(columns=['O_ID', 'O_Created_at', 'O_User', 'O_Tweet'])\n",
    "\n",
    "    for _id in tweet_ids:\n",
    "        try:\n",
    "            tweet2 = api.get_status(_id)\n",
    "        except:\n",
    "            pass\n",
    "        #print(tweet2.id_str, tweet2.created_at, tweet2.user.screen_name, tweet2.text, reply_to_tweet2)\n",
    "\n",
    "        O_id = tweet2.id_str\n",
    "        O_time = tweet2.created_at\n",
    "        O_user = tweet2.user.screen_name\n",
    "        O_text = tweet2.text\n",
    "\n",
    "        new_row = {'O_ID': O_id, 'O_Created_at': O_time, 'O_User': O_user,'O_Tweet': O_text}\n",
    "\n",
    "        df_t2= df_t2.append(new_row, ignore_index=True, sort = False)\n",
    "\n",
    "    return(df_t2)\n",
    "\n",
    "df_t2 = extract_tweet2_attributes(pages)        \n",
    "#print(df_t2)\n",
    "\n",
    "#Export collected data to csv\n",
    "#df_t2.to_csv('safaricom_tweets.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_t2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We merge the two dataframes. The unified dataframe contains the time a user made a tweet to a bank and the time the bank responded to the user. This will help us extract the response time for each bank**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df_t1, df_t2, how='outer', left_on=['Reply_to_status'], right_on = ['O_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculating time between outbound response and inbound message**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"Created_at\", \"O_Created_at\"]] = df[[\"Created_at\", \"O_Created_at\"]].apply(pd.to_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Response_time'] = df['Created_at'] - df['O_Created_at']\n",
    "\n",
    "#Convert to minutes, we only need tweets responded to in max 60 minutes\n",
    "df['Response_time'] = df['Response_time'].astype('timedelta64[s]') / 60\n",
    "df = df.loc[df['Response_time'] <= 60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time attributes\n",
    "df['Created_at_dayofweek'] = df['Created_at'].apply(lambda x: x.dayofweek)\n",
    "df['Created_at_day_of_week'] = df['Created_at'].dt.day_name()\n",
    "df['Created_at_day'] = df['Created_at'].dt.day\n",
    "df['Created_at_is_weekend'] = df['Created_at_dayofweek'].isin([5,6]).apply(lambda x: 1 if x == True else 0)\n",
    "df['Tweet_word_count'] = df.O_Tweet.apply(lambda x: len(str(x).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df[df.duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleanup any duplicates from our loop\n",
    "df.drop_duplicates(keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('banks_tweets_replies.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating Probabilities with Bayesian Modeling in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now that we have our data ready, we can apply Probabilistic Programming with PyMC3 in Python**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response time distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick check using Gaussian distribution on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "sns.distplot(df['Response_time'], kde=False)\n",
    "plt.title('Frequency of response by response time')\n",
    "plt.xlabel('Response time (minutes)')\n",
    "plt.ylabel('Number of responses');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Studentâ€™s t-distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One useful option when dealing with outliers and Gaussian distributions is to replace the Gaussian likelihood with a Studentâ€™s t-distribution. This distribution has three parameters: the mean (ðœ‡), the scale (ðœŽ) (analogous to the standard deviation), and the degrees of freedom (ðœˆ).\n",
    "- Set the boundaries of the uniform distribution of the mean to be 0 and 60.\n",
    "- ðœŽ can only be positive, therefore use HalfNormal distribution.\n",
    "- set ðœˆ as an exponential distribution with a mean of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model_t:\n",
    "    Î¼ = pm.Uniform('Î¼', lower=0, upper=60)\n",
    "    Ïƒ = pm.HalfNormal('Ïƒ', sd=10)\n",
    "    Î½ = pm.Exponential('Î½', 1/1)\n",
    "    y = pm.StudentT('y', mu=Î¼, sd=Ïƒ, nu=Î½, observed=df['Response_time'])\n",
    "    trace_t = pm.sample(2000, tune=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCMC diagnostics\n",
    "From the following trace plot, we can visually get the plausible values of ðœ‡ from the posterior.\n",
    "We should compare this result with those from the the result we obtained analytically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace_t[:1000], var_names = ['Î¼']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Response_time.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The left plot shows the distribution of values collected for ðœ‡. What we get is a measure of uncertainty and credible values of ðœ‡ between 4.5 and 10 minutes.\n",
    "- It is obvious that samples that have been drawn from distributions that are significantly different from the target distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior predictive check\n",
    "One way to visualize is to look if the model can reproduce the patterns observed in the real data. For example, how close are the inferred means to the actual sample mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppc = pm.sample_posterior_predictive(trace_t, samples=1000, model=model_t)\n",
    "_, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.hist([n.mean() for n in ppc['y']], bins=19, alpha=0.5)\n",
    "ax.axvline(df['Response_time'].mean())\n",
    "ax.set(title='Posterior predictive of the mean', xlabel='mean(x)', ylabel='Frequency');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inferred mean is so far away from the actual sample mean. This confirms that Studentâ€™s t-distribution is not a proper choice for our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Poisson distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poisson distribution is generally used to describe the probability of a given number of events occurring on a fixed time/space interval. Thus, the Poisson distribution assumes that the events occur independently of each other and at a fixed interval of time and/or space. This discrete distribution is parametrized using only one value ðœ‡, which corresponds to the mean and also the variance of the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model_p:\n",
    "    Î¼ = pm.Uniform('Î¼', lower=0, upper=60)\n",
    "    ## Define Poisson likelihood\n",
    "    y = pm.Poisson('y', mu=Î¼, observed=df['Response_time'].values)\n",
    "    trace_p = pm.sample(2000, tune=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Chain Monte Carlo (MCMC) diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace_p);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Response_time.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The measure of uncertainty and credible values of ðœ‡ is between 3.0 and 5.5 minutes. This is way better already."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autocorrelations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want the autocorrelation drops with increasing x-axis in the plot. Because this indicates a low degree of correlation between our samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = pm.autocorrplot(trace_p, var_names=['Î¼'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our samples from the Poisson model has dropped to low values of autocorrelation, which is a good sign."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Posterior predictive check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use posterior predictive check to â€œlook for systematic discrepancies between real and simulated dataâ€. There are multiple ways to do posterior predictive check, and Iâ€™d like to check if my model makes sense in various ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ppc_p = pm.sample_posterior_predictive(\n",
    "    trace_p, 100, model_p, random_seed=123)\n",
    "y_pred_p = az.from_pymc3(trace=trace_p, posterior_predictive=y_ppc_p)\n",
    "az.plot_ppc(y_pred_p, figsize=(10, 5), mean=False)\n",
    "plt.xlim(0, 60);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation:\n",
    "- The single (black) line is a kernel density estimate (KDE) of the data and the many purple lines are KDEs computed from each one of the 100 posterior predictive samples. The purple lines reflect the uncertainty we have about the inferred distribution of the predicted data.\n",
    "- From the above plot, I canâ€™t consider the scale of a Poisson distribution as a reasonable practical proxy for the standard deviation of the data even after removing outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Posterior predictive check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppc = pm.sample_posterior_predictive(trace_p, samples=1000, model=model_p)\n",
    "_, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.hist([n.mean() for n in ppc['y']], bins=19, alpha=0.5)\n",
    "ax.axvline(df['Response_time'].mean())\n",
    "ax.set(title='Posterior predictive of the mean', xlabel='mean(x)', ylabel='Frequency');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The inferred means to the actual sample mean are much closer than what we got from Studentâ€™s t-distribution. But still, there is a small gap.\n",
    "- The problem with using a Poisson distribution is that mean and variance are described by the same parameter. So one way to solve this problem is to model the data as a mixture of Poisson distribution with rates coming from a gamma distribution, which gives us the rationale to use the negative-binomial distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Negative binomial distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative binomial distribution has very similar characteristics to the Poisson distribution except that it has two parameters (ðœ‡ and ð›¼) which enables it to vary its variance independently of its mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model_n:\n",
    "    \n",
    "    Î¼ = pm.Uniform('Î¼', lower=0, upper=60)\n",
    "    Î± = pm.Uniform('Î±', lower=0, upper=100)\n",
    "    \n",
    "    y_pred = pm.NegativeBinomial('y_pred', mu=Î¼, alpha=Î±)\n",
    "    y_est = pm.NegativeBinomial('y_est', mu=Î¼, alpha=Î±, observed=df['response_time'].values)\n",
    "    \n",
    "    trace_n = pm.sample(2000, tune=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCMC diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace_n, var_names=['Î¼', 'Î±']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The measure of uncertainty and credible values of ðœ‡ is between 13.0 and 13.6 minutes, and it is very closer to the target sample mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Posterior predictive check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ppc_n = pm.sample_posterior_predictive(\n",
    "    trace_n, 100, model_n, random_seed=123)\n",
    "y_pred_n = az.from_pymc3(trace=trace_n, posterior_predictive=y_ppc_n)\n",
    "az.plot_ppc(y_pred_n, figsize=(10, 5), mean=False)\n",
    "plt.xlim(0, 60);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Negative binomial distribution in our model leads to predictive samples that seem to better fit the data in terms of the location of the peak of the distribution and also its spread."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Posterior predictive check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppc = pm.sample_posterior_predictive(trace_n, samples=1000, model=model_n)\n",
    "_, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.hist([n.mean() for n in ppc['y_est']], bins=19, alpha=0.5)\n",
    "ax.axvline(df['response_time'].mean())\n",
    "ax.set(title='Posterior predictive of the mean', xlabel='mean(x)', ylabel='Frequency');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sum it up, the following are what we get for the measure of uncertainty and credible values of (ðœ‡):\n",
    "- Student t-distribution: 7.4 to 7.8 minutes\n",
    "- Poisson distribution: 13.22 to 13.34 minutes\n",
    "- Negative Binomial distribution: 13.0 to 13.6 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Posterior predictive distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_lim = 60\n",
    "\n",
    "y_pred = trace_n.get_values('y_pred')\n",
    "mu_mean = trace_n.get_values('Î¼').mean()\n",
    "\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "fig.add_subplot(211)\n",
    "\n",
    "_ = plt.hist(y_pred, range=[0, x_lim], bins=x_lim, color=colors[1])   \n",
    "_ = plt.xlim(1, x_lim)\n",
    "_ = plt.ylabel('Frequency')\n",
    "_ = plt.title('Posterior predictive distribution')\n",
    "\n",
    "fig.add_subplot(212)\n",
    "\n",
    "_ = plt.hist(df['response_time'].values, range=[0, x_lim], bins=x_lim)\n",
    "_ = plt.xlabel('Response time in minutes')\n",
    "_ = plt.ylabel('Frequency')\n",
    "_ = plt.title('Distribution of observed data')\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posterior predictive distribution somewhat resembles the distribution of the observed data, suggesting that the Negative binomial model is a more appropriate fit for the underlying data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian methods for hierarchical modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We want to study each bank as a separated entity. We want to build a model to estimate the response time of each bank and, at the same time, estimate the response time of the entire data. This type of model is known as a hierarchical model or multilevel model.\n",
    "- My intuition would suggest that different banks has different response time. The customer service twitter response from Absa might be faster than the response from Coop for example. As such, I decide to model each bank independently, estimating parameters Î¼ and Î± for each airline.\n",
    "- One consideration is that some banks may have fewer customer inquiries from twitter than others. As such, our estimates of response time for banks with few customer inquires will have a higher degree of uncertainty than banls with a large number of customer inquiries. The below plot illustrates the discrepancy in sample size per bank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "sns.countplot(x=\"User\", data=df, order = df['User'].value_counts().index)\n",
    "plt.xlabel('Bank')\n",
    "plt.ylabel('Number of response')\n",
    "plt.title('Number of response per bank')\n",
    "plt.xticks(rotation=45);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian modeling each bank with negative binomial distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indiv_traces = {}\n",
    "\n",
    "# Convert categorical variables to integer\n",
    "le = preprocessing.LabelEncoder()\n",
    "banks_idx = le.fit_transform(df['User'])\n",
    "banks = le.classes_\n",
    "n_banks = len(banks)\n",
    "\n",
    "for p in banks:\n",
    "    with pm.Model() as model_h:\n",
    "        Î± = pm.Uniform('Î±', lower=0, upper=100)\n",
    "        Î¼ = pm.Uniform('Î¼', lower=0, upper=60)\n",
    "        \n",
    "        data = df[df['User']==p]['Response_time'].values\n",
    "        y_est = pm.NegativeBinomial('y_est', mu=Î¼, alpha=Î±, observed=data)\n",
    "\n",
    "        y_pred = pm.NegativeBinomial('y_pred', mu=Î¼, alpha=Î±)\n",
    "        \n",
    "        trace_h = pm.sample(2000, tune=2000)\n",
    "        \n",
    "        indiv_traces[p] = trace_h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Posterior predictive distribution for each bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(6,2, figsize=(12, 12))\n",
    "axs = axs.ravel()\n",
    "y_left_max = 45\n",
    "y_right_max = 1350\n",
    "x_lim = 60\n",
    "ix = [0,1,2,3,4,5]\n",
    "\n",
    "\n",
    "for i, j, p in zip([0,1,2,3,4,5], [0,2,4,6,8,10], banks[ix]):\n",
    "    axs[j].set_title('Observed: %s' % p)\n",
    "    axs[j].hist(df[df['User']==p]['Response_time'].values, range=[0, x_lim], bins=x_lim)\n",
    "    axs[j].set_ylim([0, y_left_max])\n",
    "\n",
    "for i, j, p in zip([0,1,2,3,4,5], [1,3,5,7,9,11], banks[ix]):\n",
    "    axs[j].set_title('Posterior predictive distribution: %s' % p)\n",
    "    axs[j].hist(indiv_traces[p].get_values('y_pred'), range=[0, x_lim], bins=x_lim, color=colors[1])\n",
    "    axs[j].set_ylim([0, y_right_max])\n",
    "\n",
    "axs[4].set_xlabel('Response time (minutes)')\n",
    "axs[5].set_xlabel('Response time (minutes)')\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "- Among the above the banks, xxx posterior predictive distribution vary considerably to xxx and xxx. The distribution of xxx towards right.\n",
    "- This could accurately reflect the characteristics of its customer service twitter response time, means in general it takes longer for xxx to respond than those of xxx or xxx.\n",
    "- Or it could be incomplete due to small sample size, as we have way more data from xxx than from xxx."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Hierarchical Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variables for the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['Response_time', 'User', 'Created_at_is_weekend', 'Tweet_word_count']]\n",
    "formula = 'Response_time ~ ' + ' + '.join(['%s' % variable for variable in df.columns[1:]])\n",
    "formula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code snippet, we:\n",
    "- Convert categorical variables to integer.\n",
    "- Estimate a baseline parameter value ð›½0 for each airline customer service response time.\n",
    "- Estimate all the other parameter across the entire population of the airlines in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import theano.tensor as tt\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "banks_idx = le.fit_transform(df['User'])\n",
    "banks = le.classes_\n",
    "n_banks = len(banks)\n",
    "\n",
    "with pm.Model() as model_hr:\n",
    "\n",
    "    intercept = pm.Normal('intercept', mu=0, sd=100, shape=n_banks)\n",
    "    slope_created_at_y_is_weekend = pm.Normal('slope_created_at_y_is_weekend', mu=0, sd=100)\n",
    "    slope_word_count = pm.Normal('slope_word_count', mu=0, sd=100)\n",
    "    \n",
    "    Î¼ = tt.exp(intercept[airlines_idx] \n",
    "                + slope_created_at_y_is_weekend*df.Created_at_is_weekend\n",
    "                + slope_word_count*df.Tweet_word_count)\n",
    "    \n",
    "    y_est = pm.Poisson('y_est', mu=Î¼, observed=df['Response_time'].values)\n",
    "    \n",
    "    start = pm.find_MAP()\n",
    "    step = pm.Metropolis()\n",
    "    trace_hr = pm.sample(5000, step, start=start, progressbar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCMC diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace_hr);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "- Each bank has a different baseline response time, however, several of them are pretty close.\n",
    "- If you send a request over the weekend, you would expect a marginally longer wait time before getting a response.\n",
    "- The more words on the response, the marginally longer wait time before getting a response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forest plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = pm.forestplot(trace_hr, var_names=['intercept'])\n",
    "ax[0].set_yticklabels(banks.tolist());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model estimates the above Î²0 (intercept) parameters for every bank. The dot is the most likely value of the parameter for each bank. It look like our model has very little uncertainty for every bank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppc = pm.sample_posterior_predictive(trace_hr, samples=2000, model=model_hr)\n",
    "az.r2_score(df.Response_time.values, ppc['y_est'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter notebook can be located on the Github. Link on my blog thesiliconsavannah.com."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
